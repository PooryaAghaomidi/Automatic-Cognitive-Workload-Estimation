{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "wl_reshaped_run1.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "kLdgG6oKfr7L",
    "lPsQB_1pgNuw",
    "O0Z5i2a3gARm",
    "8Kt6jnr9jtzg",
    "j_FHubP0j1xR",
    "-1zEqmuCknco"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kLdgG6oKfr7L"
   },
   "source": [
    "# **import libraries, data and label**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2ga-vebpi3Sk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c0eb3d66-506c-47d1-9371-1894207770bb"
   },
   "source": [
    "# Import necessary libraries and packages\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adagrad, SGD, Adamax, RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.layers import BatchNormalization, GlobalAveragePooling1D\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "!pip install tensorflow_addons\n",
    "from tensorflow_addons.optimizers import CyclicalLearningRate\n",
    "\n",
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "# Load label data and define constants\n",
    "mypath = '/content/drive/My Drive/valdata/'\n",
    "onlyfiles = [f for f in listdir(mypath) if (isfile(join(mypath, f)))]\n",
    "ll = [f for f in onlyfiles if f[0] == 'w']\n",
    "lbl = sio.loadmat(mypath + ll[0])['wlbl']\n",
    "lbl = np.reshape(lbl, (840,))\n",
    "size = 840\n",
    "big = 50000\n",
    "X = np.zeros((size, big))\n",
    "np.random.seed(10)\n",
    "number_of_classes = 2"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SbUyCm_qeBq1",
    "outputId": "904e2c21-2a45-4e61-fcec-f63141b7818a"
   },
   "source": [
    "# Load data and gather information\n",
    "onlyfiles = [f for f in listdir(mypath) if (isfile(join(mypath, f)))]\n",
    "mats = [f for f in onlyfiles if f[4] == 'm']\n",
    "mats = sorted(mats)\n",
    "\n",
    "# Resize data to a uniform shape\n",
    "for i in range(size):\n",
    "    # Load and reshape data\n",
    "    dummy = sio.loadmat(mypath + mats[i])['val'][0, :]\n",
    "    if (big - len(dummy)) <= 0:\n",
    "        X[i, :] = dummy[0:big]\n",
    "    else:\n",
    "        b = dummy[5000:((big - len(dummy)) + 5000)]\n",
    "        goal = np.hstack((dummy, b))\n",
    "        while len(goal) != big:\n",
    "            b = dummy[5000:((big - len(dummy)) + 5000)]\n",
    "            goal = np.hstack((goal, b))\n",
    "        X[i, :] = goal\n",
    "\n",
    "print(' Done ')\n",
    "\n",
    "# Save reshaped data to a CSV file\n",
    "np.savetxt(\"/content/drive/My Drive/valdata/X_reshaped.csv\", X, delimiter = \",\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPsQB_1pgNuw"
   },
   "source": [
    "# **data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SHTMWop_gLhv"
   },
   "source": [
    "# Function for picking the right class among outputs\n",
    "def change(x):\n",
    "    \"\"\"\n",
    "    Transforms the output predictions into single class output by selecting the class with the highest probability for each sample.\n",
    "\n",
    "    Args:\n",
    "    x (numpy.ndarray): Array of shape (number of samples, number of classes) representing predicted probabilities.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: Array of integers representing the selected classes.\n",
    "    \"\"\"\n",
    "    answer = np.zeros((np.shape(x)[0]))\n",
    "    for i in range(np.shape(x)[0]):\n",
    "        max_value = max(x[i, :])\n",
    "        max_index = list(x[i, :]).index(max_value)\n",
    "        answer[i] = max_index\n",
    "    return answer.astype(np.int)\n",
    "\n",
    "\n",
    "#windowing data\n",
    "Xnew = np.zeros((80640, 2500))\n",
    "a = -1\n",
    "for k in range(840):\n",
    "    a = a + 1\n",
    "    c = 0\n",
    "    d = 2500\n",
    "    for j in range(96):\n",
    "        Xnew[(96 * a) + j] = X[k, c:d]\n",
    "        c = c + 500\n",
    "        d = d + 500\n",
    "\n",
    "#windowing label\n",
    "lbl_new = np.zeros(80640)\n",
    "a = -1\n",
    "for k in range(840):\n",
    "    a = a + 1\n",
    "    for i in range(96):\n",
    "        lbl_new[(96 * a) + i] = lbl[k]\n",
    "\n",
    "#int labels to binary\n",
    "Label_set = np.zeros((80640, number_of_classes))\n",
    "for i in range(80640):\n",
    "    row = np.zeros((number_of_classes))\n",
    "    row[int(lbl_new[i]) - 1] = 1\n",
    "    Label_set[i, :] = row\n",
    "\n",
    "#normalization\n",
    "Xnew = (Xnew - Xnew.mean()) / (Xnew.std())\n",
    "Xnew = np.reshape(Xnew, (80640, 2500, 1))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TTXqb2tCygyw"
   },
   "source": [
    "#train and test split - 5th slot as test\n",
    "values = [i for i in range(80640)]\n",
    "permutations = np.random.permutation(values)\n",
    "Xnew = Xnew[permutations, :]\n",
    "Label_set = Label_set[permutations, :]\n",
    "\n",
    "X_train = Xnew[:int(0.8 * 80640), :]\n",
    "Y_train = Label_set[:int(0.8 * 80640), :]\n",
    "X_test = Xnew[int(0.8 * 80640):, :]\n",
    "Y_test = Label_set[int(0.8 * 80640):, :]\n",
    "sizee = len(X_test)\n",
    "\n",
    "val = 0.5\n",
    "X_val = X_test[:int(val * sizee), :]\n",
    "Y_val = Y_test[:int(val * sizee), :]\n",
    "X_test = X_test[int(val * sizee):, :]\n",
    "Y_test = Y_test[int(val * sizee):, :]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y6kHpNMQzY4h"
   },
   "source": [
    "#train and test split - 4th slot as test\n",
    "values = [i for i in range(80640)]\n",
    "permutations = np.random.permutation(values)\n",
    "Xnew = Xnew[permutations, :]\n",
    "Label_set = Label_set[permutations, :]\n",
    "\n",
    "X_train1 = Xnew[:int(0.6 * 80640), :]\n",
    "X_train2 = Xnew[int(0.8 * 80640):, :]\n",
    "X_train = np.vstack((X_train1, X_train2))\n",
    "Y_train1 = Label_set[:int(0.6 * 80640), :]\n",
    "Y_train2 = Label_set[int(0.8 * 80640):, :]\n",
    "Y_train = np.vstack((Y_train1, Y_train2))\n",
    "X_test = Xnew[int(0.6 * 80640):int(0.8 * 80640), :]\n",
    "Y_test = Label_set[int(0.6 * 80640):int(0.8 * 80640), :]\n",
    "sizee = len(X_test)\n",
    "\n",
    "val = 0.5\n",
    "X_val = X_test[:int(val * sizee), :]\n",
    "Y_val = Y_test[:int(val * sizee), :]\n",
    "X_test = X_test[int(val * sizee):, :]\n",
    "Y_test = Y_test[int(val * sizee):, :]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Pqjvovu122T4"
   },
   "source": [
    "#train and test split - 3rd slot as test\n",
    "values = [i for i in range(80640)]\n",
    "permutations = np.random.permutation(values)\n",
    "Xnew = Xnew[permutations, :]\n",
    "Label_set = Label_set[permutations, :]\n",
    "\n",
    "X_train1 = Xnew[:int(0.4 * 80640), :]\n",
    "X_train2 = Xnew[int(0.6 * 80640):, :]\n",
    "X_train = np.vstack((X_train1, X_train2))\n",
    "Y_train1 = Label_set[:int(0.4 * 80640), :]\n",
    "Y_train2 = Label_set[int(0.6 * 80640):, :]\n",
    "Y_train = np.vstack((Y_train1, Y_train2))\n",
    "X_test = Xnew[int(0.4 * 80640):int(0.6 * 80640), :]\n",
    "Y_test = Label_set[int(0.4 * 80640):int(0.6 * 80640), :]\n",
    "sizee = len(X_test)\n",
    "\n",
    "val = 0.5\n",
    "X_val = X_test[:int(val * sizee), :]\n",
    "Y_val = Y_test[:int(val * sizee), :]\n",
    "X_test = X_test[int(val * sizee):, :]\n",
    "Y_test = Y_test[int(val * sizee):, :]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y7RbQKKO3cXG"
   },
   "source": [
    "#train and test split - 2nd slot as test\n",
    "values = [i for i in range(80640)]\n",
    "permutations = np.random.permutation(values)\n",
    "Xnew = Xnew[permutations, :]\n",
    "Label_set = Label_set[permutations, :]\n",
    "\n",
    "X_train1 = Xnew[:int(0.2 * 80640), :]\n",
    "X_train2 = Xnew[int(0.4 * 80640):, :]\n",
    "X_train = np.vstack((X_train1, X_train2))\n",
    "Y_train1 = Label_set[:int(0.2 * 80640), :]\n",
    "Y_train2 = Label_set[int(0.4 * 80640):, :]\n",
    "Y_train = np.vstack((Y_train1, Y_train2))\n",
    "X_test = Xnew[int(0.2 * 80640):int(0.4 * 80640), :]\n",
    "Y_test = Label_set[int(0.2 * 80640):int(0.4 * 80640), :]\n",
    "sizee = len(X_test)\n",
    "\n",
    "val = 0.5\n",
    "X_val = X_test[:int(val * sizee), :]\n",
    "Y_val = Y_test[:int(val * sizee), :]\n",
    "X_test = X_test[int(val * sizee):, :]\n",
    "Y_test = Y_test[int(val * sizee):, :]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "viCNZ8mX3tQX"
   },
   "source": [
    "#train and test split - 1st slot as test\n",
    "values = [i for i in range(80640)]\n",
    "permutations = np.random.permutation(values)\n",
    "Xnew = Xnew[permutations, :]\n",
    "Label_set = Label_set[permutations, :]\n",
    "\n",
    "X_train = Xnew[int(0.2 * 80640):, :]\n",
    "Y_train = Label_set[int(0.2 * 80640):, :]\n",
    "X_test = Xnew[:int(0.2 * 80640), :]\n",
    "Y_test = Label_set[:int(0.2 * 80640), :]\n",
    "sizee = len(X_test)\n",
    "\n",
    "val = 0.5\n",
    "X_val = X_test[:int(val * sizee), :]\n",
    "Y_val = Y_test[:int(val * sizee), :]\n",
    "X_test = X_test[int(val * sizee):, :]\n",
    "Y_test = Y_test[int(val * sizee):, :]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0Z5i2a3gARm"
   },
   "source": [
    "# **Functions**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EU3qtqQ_jK8O"
   },
   "source": [
    "#Plot run properties\n",
    "def pretty_plot(history, field, fn):\n",
    "    def plot(data, val_data, best_index, best_value, title):\n",
    "        plt.plot(range(1, len(data) + 1), data, label = 'train')\n",
    "        plt.plot(range(1, len(data) + 1), val_data, label = 'validation')\n",
    "        if not best_index is None:\n",
    "            plt.axvline(x = best_index + 1, linestyle = ':', c = \"#777777\")\n",
    "        if not best_value is None:\n",
    "            plt.axhline(y = best_value, linestyle = ':', c = \"#777777\")\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel(field)\n",
    "        plt.xticks(range(0, len(data), 20))\n",
    "        plt.title(title)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    data = history.history[field]\n",
    "    val_data = history.history['val_' + field]\n",
    "    tail = int(0.15 * len(data))\n",
    "\n",
    "    best_index = fn(val_data)\n",
    "    best_value = val_data[best_index]\n",
    "\n",
    "    plot(data, val_data, best_index, best_value, \"{} over epochs (best {:06.4f})\".format(field, best_value))\n",
    "    plot(data[-tail:], val_data[-tail:], None, best_value, \"{} over last {} epochs\".format(field, tail))\n",
    "\n",
    "\n",
    "#learning rate finder\n",
    "from keras.callbacks import Callback\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LRFinder(Callback):\n",
    "    \"\"\"\n",
    "    Up-to date version: https://github.com/WittmannF/LRFinder\n",
    "    Example of usage:\n",
    "        from keras.models import Sequential\n",
    "        from keras.layers import Flatten, Dense\n",
    "        from keras.datasets import fashion_mnist\n",
    "        !git clone https://github.com/WittmannF/LRFinder.git\n",
    "        from LRFinder.keras_callback import LRFinder\n",
    "        # 1. Input Data\n",
    "        (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
    "        mean, std = X_train.mean(), X_train.std()\n",
    "        X_train, X_test = (X_train-mean)/std, (X_test-mean)/std\n",
    "        # 2. Define and Compile Model\n",
    "        model = Sequential([Flatten(),\n",
    "                            Dense(512, activation='relu'),\n",
    "                            Dense(10, activation='softmax')])\n",
    "        model.compile(loss='sparse_categorical_crossentropy', \\\n",
    "                      metrics=['accuracy'], optimizer='sgd')\n",
    "        # 3. Fit using Callback\n",
    "        lr_finder = LRFinder(min_lr=1e-4, max_lr=1)\n",
    "        model.fit(X_train, y_train, batch_size=128, callbacks=[lr_finder], epochs=2)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, min_lr, max_lr, mom=0.9, stop_multiplier=None,\n",
    "                 reload_weights=True, batches_lr_update=5):\n",
    "        self.min_lr = min_lr\n",
    "        self.max_lr = max_lr\n",
    "        self.mom = mom\n",
    "        self.reload_weights = reload_weights\n",
    "        self.batches_lr_update = batches_lr_update\n",
    "        if stop_multiplier is None:\n",
    "            self.stop_multiplier = -20 * self.mom / 3 + 10  # 4 if mom=0.9\n",
    "            # 10 if mom=0\n",
    "        else:\n",
    "            self.stop_multiplier = stop_multiplier\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        p = self.params\n",
    "        try:\n",
    "            n_iterations = p['epochs'] * p['samples'] // p['batch_size']\n",
    "        except:\n",
    "            n_iterations = p['steps'] * p['epochs']\n",
    "\n",
    "        self.learning_rates = np.geomspace(self.min_lr, self.max_lr, \\\n",
    "                                           num = n_iterations // self.batches_lr_update + 1)\n",
    "        self.losses = []\n",
    "        self.iteration = 0\n",
    "        self.best_loss = 0\n",
    "        if self.reload_weights:\n",
    "            self.model.save_weights('tmp.hdf5')\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        loss = logs.get('loss')\n",
    "\n",
    "        if self.iteration != 0:  # Make loss smoother using momentum\n",
    "            loss = self.losses[-1] * self.mom + loss * (1 - self.mom)\n",
    "\n",
    "        if self.iteration == 0 or loss < self.best_loss:\n",
    "            self.best_loss = loss\n",
    "\n",
    "        if self.iteration % self.batches_lr_update == 0:  # Evaluate each lr over 5 epochs\n",
    "\n",
    "            if self.reload_weights:\n",
    "                self.model.load_weights('tmp.hdf5')\n",
    "\n",
    "            lr = self.learning_rates[self.iteration // self.batches_lr_update]\n",
    "            K.set_value(self.model.optimizer.lr, lr)\n",
    "\n",
    "            self.losses.append(loss)\n",
    "\n",
    "        if loss > self.best_loss * self.stop_multiplier:  # Stop criteria\n",
    "            self.model.stop_training = True\n",
    "\n",
    "        self.iteration += 1\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.reload_weights:\n",
    "            self.model.load_weights('tmp.hdf5')\n",
    "\n",
    "        plt.figure(figsize = (12, 6))\n",
    "        plt.plot(self.learning_rates[:len(self.losses)], self.losses)\n",
    "        plt.xlabel(\"Learning Rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.xscale('log')\n",
    "        plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Kt6jnr9jtzg"
   },
   "source": [
    "# **model structure**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GCM8QRuuqHYL"
   },
   "source": [
    "# Define a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# First set of Conv1D, Pooling, and Batch Normalization layers\n",
    "model.add(Conv1D(32, 4, strides = 1, padding = 'same', activation = 'relu', input_shape = (2500, 1)))\n",
    "model.add(Conv1D(32, 4, strides = 1, padding = 'same', activation = 'relu'))\n",
    "model.add(AveragePooling1D(4, 2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Second set of Conv1D, Pooling, and Batch Normalization layers\n",
    "model.add(Conv1D(64, 4, strides = 1, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv1D(64, 4, strides = 1, padding = 'same', activation = 'relu'))\n",
    "model.add(AveragePooling1D(4, 2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Third set of Conv1D, Pooling, and Batch Normalization layers\n",
    "model.add(Conv1D(128, 4, strides = 1, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv1D(128, 4, strides = 1, padding = 'same', activation = 'relu'))\n",
    "model.add(AveragePooling1D(4, 2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Fourth set of Conv1D, Pooling, and Batch Normalization layers\n",
    "model.add(Conv1D(256, 4, strides = 1, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv1D(256, 4, strides = 1, padding = 'same', activation = 'relu'))\n",
    "model.add(AveragePooling1D(4, 2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "# Global Average Pooling layer\n",
    "model.add(GlobalAveragePooling1D())\n",
    "\n",
    "# Fully connected layers\n",
    "model.add(Dense(256, kernel_initializer = 'normal', activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, kernel_initializer = 'normal', activation = 'softmax'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_FHubP0j1xR"
   },
   "source": [
    "# **train model**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qbLcmXGUu50f"
   },
   "source": [
    "# Define the optimizer for the model\n",
    "optimizer = keras.optimizers.Adamax(lr = 0.001)\n",
    "\n",
    "# Compile the model with the specified optimizer, loss function, and metrics\n",
    "model.compile(optimizer = optimizer, loss = 'mse', metrics = ['accuracy'])\n",
    "\n",
    "# Define the learning rate finder\n",
    "lr_finder = LRFinder(min_lr = 1e-12, max_lr = 1e+1)\n",
    "\n",
    "# Train the model to find the learning rate using the lr_finder\n",
    "model.fit(X_train, Y_train, batch_size = 128, callbacks = [lr_finder], epochs = 5)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MxFVEvymqJRd"
   },
   "source": [
    "# Set up the optimizer\n",
    "opt = Adamax(learning_rate = 0.001, decay = 0.001, clipvalue = 0.5, epsilon = 1e-07)\n",
    "\n",
    "# Compile the model with the specified optimizer, loss function, and metrics\n",
    "model.compile(loss = 'mse', optimizer = opt, metrics = ['accuracy'])\n",
    "\n",
    "# Define the path for saving the trained model\n",
    "pathh = '/content/drive/My Drive/Conv_models'\n",
    "\n",
    "# Set up a ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpointer = ModelCheckpoint(filepath = pathh, monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only = True)\n",
    "\n",
    "# Train the model with specified parameters and save the training history\n",
    "hist = model.fit(X_train, Y_train, validation_data = (\n",
    "X_val, Y_val), batch_size = 128, epochs = 280, verbose = 2, shuffle = True, callbacks = [checkpointer])\n",
    "\n",
    "# Save the training history to a CSV file for analysis\n",
    "pd.DataFrame(hist.history).to_csv(path_or_buf = '/content/drive/My Drive/Conv_models/History.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1zEqmuCknco"
   },
   "source": [
    "# **train information**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "neI_Jh4Krtos",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "a6017998-8908-45ff-cf1b-4b26cb0272f5"
   },
   "source": [
    "#plot run info\n",
    "pretty_plot(hist, 'loss', lambda x: np.argmin(x))\n",
    "pretty_plot(hist, 'accuracy', lambda x: np.argmax(x))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ry2w3xlosikV",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "67f87ba9-4d03-48bd-9b68-8eaedd5cd098"
   },
   "source": [
    "#test\n",
    "testmodel = load_model('/content/drive/My Drive/Conv_models')\n",
    "tst_loss, tst_acc = testmodel.evaluate(X_test, Y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xiJyeZNUyuTA"
   },
   "source": [
    "#print model summary\n",
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ld1uwvFWyvDf"
   },
   "source": [
    "# Initialize arrays to hold evaluation metrics\n",
    "accuracy_model = np.zeros((1, 1))\n",
    "Sensitivity_model = np.zeros((1, 1))\n",
    "Specificity_model = np.zeros((1, 1))\n",
    "\n",
    "# Predict the classes using the trained model\n",
    "Y_pred = model.predict(X_test, verbose = 0, max_queue_size = 10, workers = 1, use_multiprocessing = False)\n",
    "\n",
    "# Calculate the confusion matrix for the model's predictions\n",
    "cm = confusion_matrix(change(Y_test), change(Y_pred))\n",
    "\n",
    "# Calculate various performance metrics\n",
    "accuracy_model = accuracy_score(change(Y_test), change(Y_pred))\n",
    "Sensitivity_model = cm[0, 0] / (cm[0, 0] + cm[0, 1])\n",
    "Specificity_model = cm[1, 1] / (cm[1, 0] + cm[1, 1])\n",
    "\n",
    "# Calculate the Area Under the ROC Curve (ROC AUC)\n",
    "auc = roc_auc_score(Y_test, Y_pred, average = 'macro', sample_weight = None, max_fpr = None, multi_class = 'raise', labels = None)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print('\\n\\n\\n\\n roc_auc_score : \\n', auc)\n",
    "print('\\n\\n confusion_matrix : \\n', cm)\n",
    "print('\\n\\n test accuracy is :', accuracy_model)\n",
    "print(' Sensitivity :', Sensitivity_model)\n",
    "print(' Specificity :', Specificity_model)\n",
    "print('\\n\\n classification report : \\n', classification_report(change(Y_test), change(Y_pred)))"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
